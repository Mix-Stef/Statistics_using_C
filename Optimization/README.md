Implementation of basic optimization methods, like gradient descent. To be used for the minimization of cost function like MSE when doing Linear Regression.

* Gradient Descent: An algorithm for minimizing a real (multivariable) function $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ (objective function). Given an initial guess, $x_{0}$, we use the following update rule: $$x_{t+1} = x_{t} - \nabla f$$
